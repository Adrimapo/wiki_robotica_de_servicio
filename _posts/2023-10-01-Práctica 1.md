---
title: "Práctica 1"

categories:
  - Weekly Log
tags:
  - github pages
  - ROS2
  - Robótica de servicio
---

# Localized Vacuum Cleaner

## Index:
* [Main idea of the algorithm](#idea-principall-del-algoritmo)
* [State machine](#máquina-de-estados)
* [GetPath](#clase-getpath)
* [Location](#localización)
* [Turn](#turn)
* [Forward](#forward)
* [Failed implementations and encountered problems.](#implementaciones-fallidas-y-problemas-encontrados)
* [Failure detection](#detección-de-fallo)
* [Final result](#resultado-final)


## Main idea of the algorithm
1. Robot self-localization for determining its current position.
2. Map decomposition into cells.
3. Applying dilation to obstacles.
4. Travel between nodes.


## State machine
The problem was approached using a state machine.

<figure class="align-center" style="width:50%">
  <img src="{{ site.url }}{{ site.baseurl }}/assets/images/post1/maquinaEstados.png" alt="">
  <figcaption>Basic infraestructure</figcaption>
</figure>

* **Get Path:** First, the robot's position will be mapped to the pixel it is on in the image. Then, this initial position on the image will be passed to the "get_path" method of the pathPlanner class, and this path will be optimized.

* **Get next step:** If the route has not been completed, the next 2 points of the route will be obtained.

* **Turning:** Place the robot in position.

* **Forward:** Advance while correcting the turning error.

---
---

## GetPath class
It will be responsible for obtaining the map image, dividing the map into cells, and finding the most optimal path to cover all the cells.

#### BSA algorithm
The cleaning algorithm will involve moving in four directions: north, east, south, and west. The order is important because if a cell is clear in the direction of the first, the robot will move in that direction; if not, it will move to the next one.

#### Map representation: 
The map will be divided into grids, and a matrix will be created such that grid[X][Y] = state. The state will indicate whether the cell is an obstacle, has already been traversed, or is clear. Four types of cells need to be highlighted:

* Obstacles
* Virtual obstacles (the robot has already passed through that coordinate)
* Return points
* Critical points (all surrounding cells are obstacles)

Initially, the idea of using three separate lists to store obstacles, virtual obstacles, and return points was considered. However, it was found to be more effective to maintain a single list specifically for return points, as constantly checking multiple lists could be inefficient.


#### Recovery at critical points

As the robot moves, return points are saved in a stack. Once the robot reaches a critical point, it will return to the last point saved in the stack. This return path will be calculated using gradient descent, reusing the approach from [Practice 4](https://portanova2002.wixsite.com/robotica-movil)-

#### Development outside of Unibotics
To develop this part, the map and OpenCV were used to create a small visualizer for the algorithm. This allowed for selecting the desired speed and visualizing the route the robot would take step by step.

<figure class="align-center" style="width:70%">
  <img src="{{ site.url }}{{ site.baseurl }}/assets/images/post1/visualizador.png" alt="">
  <figcaption>openCV Visualizer</figcaption>
</figure>

* **Pink Point:** Vacuum cleaner.
* **Dark Blue Cells:** Visited cells.
* **Grayish-Blue Cells:** Cells visited with the return algorithm.
* **Yellow Cells:** Return points to which the vacuum cleaner is moving.
* **Final Red Points:** Reference walls.

[Simulation (youtube)](https://www.youtube.com/shorts/l_NFTMDZLw0)


#### Route optimization
To improve the performance of the linear velocity PID, a function was created to optimize the route. If the robot moves in the same direction, the first and last cell in that direction is stored, allowing for the insertion of total distances until a change of position.

---
---

## Location
For localization, multiple points on the map were taken, and linear regression was used to calculate the necessary transformations to go from the real world to the 3D map.

<figure class="align-center" style="width:70%">
  <img src="{{ site.url }}{{ site.baseurl }}/assets/images/post1/puntos.png" alt="">
  <figcaption>Obtained points</figcaption>
</figure>

**Regression result:** 
* x = 581.237 - 101*Px
* y = 428.77 + 99*Px 

---
---

## Turn

For turning, code from the mentioned previous practice was reused. Some key points to highlight include:

* "The most challenging part was calculating the angular velocity to determine the direction to go. The arctangent of the next point in the path relative to the car's position was calculated. This value had a range from -π to π, which was converted from 0 to 2π. The same conversion was applied to the car's yaw."

* "Finally, both values were subtracted, and by applying a conversion of this value with the maximum desired angular velocity, a satisfactory behavior in the car was achieved."

With these calculations encapsulated in a function, a small PD control was added to ensure the robot doesn't have an overshoot at the beginning. As it approaches the desired orientation, the robot gradually reduces its velocity until it reaches the desired position with an error of 0.1. The error is the difference between one of the angle constants (North, South, East, West) and the current orientation.


---
---

### Forward 
To implement forward movement, a PID controller was also used in combination with an arrival tolerance. The error was adjusted for each direction to add robustness to the code. In case the vacuum cleaner overshoots the desired point, it can return to it. Additionally, as the robot moves forward, it compensates for any turning error that may occur. This turning error is obtained as in normal turning but is multiplied by a number between 5 and 10. When the error is close to 0, it won't turn enough to keep the robot from deviating, but if multiplied by a high number, it will have an overshoot and oscillate, functioning like the proportional part of a PID controller.

### Obtención de imagen y erosión
For obtaining the image, the urllib library was used to make the function work both for Unibotics and local testing. An erosion process was added to enlarge the obstacles and prevent the robot from unnecessarily getting too close to them.

---
---

## Failed Implementations and Encountered Problems
### Smoothed Turning

An attempt was made to implement turning in such a way that the robot starts turning toward the next direction before reaching the point. However, this caused the issue that sometimes, during the turn, the point was lost, and the turning point was set near the state change threshold. While the early turn wasn't very noticeable, it caused the robot to lose its position.


### Position error
When calculating the points for linear regression, I noticed an error in the position. At first, I thought it was proportional to time, so I calculated the cumulative error of each component every second to compensate for it in the vacuum cleaner class. The problem arose when I discovered that the error was not linear with respect to time. While this technique mitigated the effect, the error was still significant enough to cause the vacuum cleaner to collide.

### Position error 2
As a second possible way to solve the problem, a few ideas were considered:
1. **VFF Algorithm:** Using a potential field algorithm to move away from walls. However, this could result in the points gradually shifting away from their desired positions, making it challenging to reach the target point.

2. **Square Test:** Implementing a square test to adjust the position.

3. **Self-Localization Algorithm:** Leveraging the laser sensor, an algorithm for self-localization was considered. By estimating the positions of nearby obstacles on the map from the current point, comparing the map distance to the obstacle with the real laser-based distance, it would be possible to calculate the error at each moment and update the position for accurate robot localization.

#### Self-Localization
The third option will be chosen. A possible implementation could involve using the get_next_point function to calculate the nearest obstacles within a certain threshold in specific directions (North, South, East, West). Once the robot reaches a position, the accumulated error will be calculated and subtracted, allowing the robot to dynamically correct the error. Additionally, since the temporal error had favorable results over having none, both techniques will be combined. This means that if, at any point, the robot doesn't detect obstacles within its threshold, the error will be gradually corrected.

#### Probabilistic Sensor Observation Model.
We will have the 2D map providing the positions of obstacles, and laser readings will adjust the robot's position. The laser will take measurements within a specific aperture to locate only the obstacles that are directly in front of the robot, where the reference walls are expected to be.

```python
# North
  errorY = wall[1] - (position[1] - laser)
  errorX = wall[0] - position[0] 

# East
  errorX = wall[0] - (position[0] - laser)
  errorY = position[1] - wall[1]

# Error update
  self.errorY += errorY
  self.errorX += errorX
```

#### Probabilistic Fusion
The robot's position will be continually updated by calculating the accumulated error until it reaches a point. This error will be gradually added to the total position, with a certain percentage added each time to ensure that the change is not immediate and accumulates over time. In this case, the implementation functions like the integral component of a PID controller to compensate for the accumulated error.

```python
  x = HAL.getPose3d().x + self.errorX * PX
  y = HAL.getPose3d().y + self.errorY * PY
```

#### Test in the environment
An implementation was carried out using the route testing program to obtain the exact point of reference walls. The route in execution is the optimized route.

[Test in enviroment (Youtube)](https://www.youtube.com/shorts/OUknfT2OJ-M)


---
---

## Failure detection:
In the end, the problem was much simpler than initially thought. The speed control is performed by a PID, and when the robot was positioned to make the required turn, a small error was left to avoid excessive delay in the turning process. The issue was that when advancing, the robot deviated due to this small error. The robot was turning, but since the error was close to zero, it wasn't enough to prevent deviation. Multiplying this error to increase the correction yielded much more favorable results, allowing the exercise to be completed satisfactorily and eliminating the complexity of the previous section. In the final version, the wall localization implementation was omitted because it added complexity and computational time for a minor error correction.

---
---

## Final result
* The first video shows a more efficient but rushed approach. After 20 minutes, the platform got stuck, so this was the initial result. In this map, you can also see the reference points on the wall, although they are not used.

[Full simulated path(youtube)](https://www.youtube.com/watch?v=s6tZkR5K7w4)

[Full path(youtube)](https://www.youtube.com/watch?v=0Hdh18cxAn4)



* In this implementation, the cell size is smaller to prevent collisions, we increased erosion, and tested the program's robustness.

[Precise path(youtube)](https://www.youtube.com/watch?v=oVQQRuzgxb8)
