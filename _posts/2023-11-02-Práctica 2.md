---
title: "Post 2"

categories:
  - Weekly Log
tags:
  - ROS2
  - Service robotics
---

# Rescue people

## Index:
* [Main idea of the algorithm](#main-idea-of-the-algorithm)
* [State machine](#state-machine)
* [Initial coordinates](#initial-coordinates)
* [Speed control](#speed-control)
* [Face detection](#face-detection)

---
---

## State machine
<figure class="align-center" style="width:70%">
  <img src="{{ site.url }}{{ site.baseurl }}/assets/images/post2/machineState.png" alt="">
  <figcaption>Drone machine state</figcaption>
</figure>

***

### Take off
Will be responsible for taking off the drone, once in the takeoff state, it will proceed to mode 'go to point'.

### Go to point
In the first execution, it will go to the point where it is estimated the survivors are located, the control will be at an intermediate level, in terms of speed.

### Search people
It will describe a spiral movement while using the detection algorithm. Face detection could also be performed while heading to the point, but since this process requires a lot of computational power, and the simulator didn't meet the necessary performance requirements to avoid deviating from the go-to point control, it was decided to only search for faces in this mode. The spiral was created by increasing the time and using the following pattern:

<figure class="align-center" style="width:70%">
  <img src="{{ site.url }}{{ site.baseurl }}/assets/images/post2/spiral.png" alt="">
  <figcaption>Drone machine state</figcaption>
</figure>


### Come back home / land
An estimated battery time is calculated, and once the battery level drops to 10%, the robot will return to the base. It can then take off again and head to the position of the last survivor. The issue was that after landing the drone, when the take-off command was issued, the drone either didn't launch or the simulation was halted. To solve this, it was decided to search for all survivors in a single round.

---
---

## Initial coordinates:
First, we convert the coordinates to mapping and easting, and then we will use the last 3 points as local reference for the simulation map.

* Initial pose: 
  * Initial pose = 40º16’48.2” N, 3º49’03.5W
    * Easting = 430492        
    * Northing = 4459162
  * Map pose = 492, 162


* Survivors pose
  * Survivors pose = 40º16’47.23” N, 3º49’01.78” W
    * Easting = 430532 
    * Northing = 4459132
  * Map pose = 532, 132

---
---

## Speed control
For speed control, we will use two proportional controllers: angular and linear. The linear 'p' value will be small since we will be moving over large distances, and it's important to slow down as we approach. The linear speed will also depend on the angular speed, which will be adjusted so that it's in the same dimension as the linear speed. If the angular speed is at its maximum, the linear speed will be 0, and as we reduce angular speed, the drone will have more linear speed.

---
---

## Face detection
To locate the faces of the survivors, we will use the face_cascade. We will iterate between 0 and 360 degrees in 15-degree increments because this neural network requires the image to be straight. The problem is that this will add more computational load to the program. This part was initially developed locally.

To save the survivors, the following transformation is used, estimated with pixels. Having a 3D camera with intrinsic and extrinsic parameters would allow for a more precise approximation, estimating the real position based on the pixel position. Each time a face is detected, it is checked whether it is within a 3-unit threshold of the estimated positions of each survivor. If the position is not added, it will be displayed in blue; if it has already been added, it will be displayed in green.

<figure class="align-center" style="width:70%">
  <img src="{{ site.url }}{{ site.baseurl }}/assets/images/post2/rotate.gif" alt="Estimation">
</figure>


---
---


## Final result
[Mission trial 1](https://www.youtube.com/watch?v=yqIwPKJN_Nw)

[Mission trial 2](https://www.youtube.com/watch?v=DWeU3IYiSFE)